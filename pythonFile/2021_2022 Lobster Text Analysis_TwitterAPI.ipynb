{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a766b872",
   "metadata": {},
   "source": [
    " # TWITTER SCRAPPING DATASETS LOADING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62db83f4",
   "metadata": {},
   "source": [
    "## TOKEN ACCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f114db86",
   "metadata": {},
   "outputs": [],
   "source": [
    "TWITTER_CONSUMER_KEY = 'IXBsOhx5JVqDH4WiScZnHvJC8'\n",
    "TWITTER_CONSUMER_SECRET = '8rSBkRBy5CxE1As9S64hnHjctSPkUmZ2GnTvL3MNjoY2DTPVfi'\n",
    "TWITTER_ACCESS_TOKEN = '1536977498650685440-3pCXHUtEx3m0A9MZji6yWUDQBOceUW'\n",
    "TWITTER_ACCESS_TOKEN_SECRET = 'JHInPSzHxfd4tdtAp9TyfxDcjKre8H8Y0aGJczVknBoL6'\n",
    "BEARER_TOKEN = 'AAAAAAAAAAAAAAAAAAAAAP0kdwEAAAAA5caCys8AnDktaOytX5szfYD%2FWms%3DsPZBH2OCUXtPR4feADG7sos4frqL9BBj3cGjivJZ0TnpYCSo4A'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dba0dbe",
   "metadata": {},
   "source": [
    "## Importation of Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc6c6173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in c:\\users\\sarad\\anaconda3\\lib\\site-packages (4.13.0)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in c:\\users\\sarad\\anaconda3\\lib\\site-packages (from tweepy) (1.3.1)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in c:\\users\\sarad\\anaconda3\\lib\\site-packages (from tweepy) (2.28.1)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in c:\\users\\sarad\\anaconda3\\lib\\site-packages (from tweepy) (3.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sarad\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\sarad\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sarad\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sarad\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (1.26.11)\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy\n",
    "\n",
    "import tweepy\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import collections\n",
    "from collections import Counter\n",
    "import itertools "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569cf398",
   "metadata": {},
   "source": [
    "## Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61301a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def authenticate():\n",
    "    auth = tweepy.OAuthHandler(TWITTER_CONSUMER_KEY, TWITTER_CONSUMER_SECRET)\n",
    "    auth.set_access_token(TWITTER_ACCESS_TOKEN, TWITTER_ACCESS_TOKEN_SECRET)    \n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "    return api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1721de",
   "metadata": {},
   "source": [
    "## DICTIONNARY SET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7516f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dictionnary = \"(écologie OR énergie OR éolienne OR éléctricité OR transport OR nucléaire OR co2 OR carbonne OR petrole OR renouvelables OR énergies renouvelables OR chauffage électrique OR énergie fossiles OR gaz OR hydraulique OR climat OR solaire OR effet de serre OR énergie primaire OR eéologique OR énergitique OR ecologique OR recyclage OR biodiversite OR biomasse OR développement durable OR écocitoyen OR écosysteme) lang:fr\"\n",
    "word_dictionnary_wordcloud = [\"écologie\" , \"énergie\" , \"éolienne\" , \"éléctricité\" , \"transport\" , \"nucléaire\" , \"co2\" , \"carbone\" , \"petrole\" , \"renouvelables\" , \"énergies renouvelables\" , \"chauffage\" \"électrique\" , \"énergies fossiles\" , \"gaz\" , \"hydraulique\" , \"climat\" , \"solaire\" , \"effet de serre\" , \"énergie primaire\" , \"écologique\" , \"énergitique\" , \"ecologique\" , \"recyclage\" , \"biodiversite\" , \"biomasse\" , \"développement durable\" , \"écocitoyen \",\"écosysteme\", 'Nucléaire','CNR','Transition','Développement','Chaleur',\n",
    "'énergétique','#TransitionEnergétique','IFPEN','Framatome','EDF','Energy','déchets','énergie','biomasse','#energierenouvelable','Energies','Renouvelable','l’énergie','photovoltaïque','centrales','carbone','électrique','climat' , '#COP26' ,                          'consommation','#Nucléaire','gaz']\n",
    "actor_dictionnary = [\"IFPENinnovation\", \"Ecologie_Gouv\", \"EU_Commission\", \"ARP_NUC\",\"Framatome_\", \"ENGIE_EC\", \"EDF_Entreprises\", \"elonmusk\", \"Arkolia_EnR\",\"EnergieCap\",\"CNR_Officiel\", \"Groupe_Coriance\",\"DalkiaWEnergy\",\"amisdelaterre\",\"Enercoop_SCIC\", \"ademe\", \"Novethic\", \"Ecologie_Gouv\", \"lemonde_planete\", \"FNEasso\", \"Alternatiba_\", \"EDFofficiel\", \"ecosia\", \"elonmusk\", \"GretaThunberg\",\"Youth4Climatefr\", \"McKinsey_France\",\"J_B_Levy\", \"PPouyanne\", \"sdnfr\", \"greenpeacefr\", \"WWFFrance\", \"JMJancovici\", \"nouel_juliette\", \"huetsylvestre\", \"NicolasMeilhan\", \"audreygarric\", \"valmasdel\", \"alaingrandjean\", \"theShiftPR0JECT\", \"youmatter_fr\",\"IFPENinnovation\", \"EU_Commission\", \"Framatome_\", \"ENGIE_EC\", \"EDF_Entreprises\", \"Albioma\", \"Arkolia_EnR\",\"EnergieCap\",\"CNR_Officiel\", \"Groupe_Coriance\",\"DalkiaWEnergy\", \n",
    "\n",
    "                    \"CFigueres\" ,\n",
    "                    \"yjado\" ,\n",
    "                    \"algore\" ,\n",
    "                \"CCNUCC\",\n",
    "                   \n",
    "                         \"RACFrance\", \n",
    "                    \n",
    "                ] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bba7822",
   "metadata": {},
   "source": [
    "## 1ST DATASET : SEARCH TWEETS FOLLOWING THE WORD DICTIONNARY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf1cdb2",
   "metadata": {},
   "source": [
    "### GETTER OF 1ST DATASET'S ATTRIBUTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1fb9e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getretweet(id):\n",
    "    api = authenticate()\n",
    "    status = api.get_status(id)\n",
    "    retweet_count = status.retweet_count       \n",
    "    return retweet_count\n",
    "\n",
    "def getClient():\n",
    "    client = tweepy.Client(bearer_token=BEARER_TOKEN)\n",
    "    return client\n",
    "\n",
    "def getTweet(client,id):\n",
    "    tweet = client.get_tweet(id, expansions = ['author_id'], user_fields= ['username'])\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f004ff74",
   "metadata": {},
   "source": [
    "### SEARCHTWEET OF 1ST DATABASE WITH WORD DICTIONNARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "376a59e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "client = getClient()\n",
    "\n",
    "def searchTweetsWordDictionnary(client, query):\n",
    "    \n",
    "    tweets = client.search_recent_tweets(query=query, tweet_fields=['context_annotations','created_at'], max_results=100)\n",
    "    tweet_data = tweets.data  \n",
    "    pagination_tweets = tweepy.Paginator(client.search_recent_tweets, query=query,\n",
    "                                  tweet_fields=['context_annotations', 'created_at'], max_results=100).flatten(limit=300)\n",
    "    if not tweet_data is None and len(tweet_data) > 0:\n",
    "        for tweet in pagination_tweets :\n",
    "            twt = getTweet(client,tweet['id'])\n",
    "            username= twt.includes['users'][0].username\n",
    "            retweet= getretweet(tweet.id)   \n",
    "            id = tweet.id\n",
    "            text = tweet.text\n",
    "            date = tweet.created_at        \n",
    "            line = {'USER_ID' : id, 'DATE_TIME' : date, 'USERNAME' :username , 'TWEET_TEXT':text, 'NUMBER_OF_RETWEET': retweet }                     \n",
    "            results.append(line)\n",
    "    else:\n",
    "        return ''\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b8d537",
   "metadata": {},
   "source": [
    "### PRINT IN A DATAFRAME TO EXPORT 1ST DATABASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf12115b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USER_ID</th>\n",
       "      <th>DATE_TIME</th>\n",
       "      <th>USERNAME</th>\n",
       "      <th>TWEET_TEXT</th>\n",
       "      <th>NUMBER_OF_RETWEET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1649730716140204035</td>\n",
       "      <td>2023-04-22 11:03:47+00:00</td>\n",
       "      <td>ppedrito64</td>\n",
       "      <td>@EmmanuelMacron Cessez de mentir. Sortez l’éle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1649730714210795521</td>\n",
       "      <td>2023-04-22 11:03:47+00:00</td>\n",
       "      <td>patrick27190</td>\n",
       "      <td>RT @ChamboncelBen: \" C'est inadmissible\"\\n\\nLa...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1649730710758981632</td>\n",
       "      <td>2023-04-22 11:03:46+00:00</td>\n",
       "      <td>dyzatch</td>\n",
       "      <td>RT @Europhobe: 7 milliards, c'est le prix d'un...</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1649730707634237441</td>\n",
       "      <td>2023-04-22 11:03:45+00:00</td>\n",
       "      <td>Colombe_w</td>\n",
       "      <td>RT @DigitalGanon: Leurs vieilles ganache de bo...</td>\n",
       "      <td>271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1649730706342309888</td>\n",
       "      <td>2023-04-22 11:03:45+00:00</td>\n",
       "      <td>GalaT77</td>\n",
       "      <td>RT @Reporterre: Plus de routes ne signifie pas...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>1649729672912027650</td>\n",
       "      <td>2023-04-22 10:59:39+00:00</td>\n",
       "      <td>GillardFabien</td>\n",
       "      <td>RT @elkevdbrandt: Plus de trains 🚊 , vers et d...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>1649729670252748801</td>\n",
       "      <td>2023-04-22 10:59:38+00:00</td>\n",
       "      <td>Josianels3</td>\n",
       "      <td>RT @f_philippot: Pourquoi dit-on qu’on va manq...</td>\n",
       "      <td>1679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>1649729661755064321</td>\n",
       "      <td>2023-04-22 10:59:36+00:00</td>\n",
       "      <td>ClaireMeilleray</td>\n",
       "      <td>RT @JLMTV_INFOS: Bonjour Twitter. \\nHier le mi...</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>1649729660383576065</td>\n",
       "      <td>2023-04-22 10:59:36+00:00</td>\n",
       "      <td>stephaneLetell7</td>\n",
       "      <td>RT @f_philippot: La seule raison pour laquelle...</td>\n",
       "      <td>343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>1649729655564279814</td>\n",
       "      <td>2023-04-22 10:59:35+00:00</td>\n",
       "      <td>jeanberube4</td>\n",
       "      <td>RT @PatriceRoyTJ: La fonte des glaciers bat de...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 USER_ID                 DATE_TIME         USERNAME  \\\n",
       "0    1649730716140204035 2023-04-22 11:03:47+00:00       ppedrito64   \n",
       "1    1649730714210795521 2023-04-22 11:03:47+00:00     patrick27190   \n",
       "2    1649730710758981632 2023-04-22 11:03:46+00:00          dyzatch   \n",
       "3    1649730707634237441 2023-04-22 11:03:45+00:00        Colombe_w   \n",
       "4    1649730706342309888 2023-04-22 11:03:45+00:00          GalaT77   \n",
       "..                   ...                       ...              ...   \n",
       "295  1649729672912027650 2023-04-22 10:59:39+00:00    GillardFabien   \n",
       "296  1649729670252748801 2023-04-22 10:59:38+00:00       Josianels3   \n",
       "297  1649729661755064321 2023-04-22 10:59:36+00:00  ClaireMeilleray   \n",
       "298  1649729660383576065 2023-04-22 10:59:36+00:00  stephaneLetell7   \n",
       "299  1649729655564279814 2023-04-22 10:59:35+00:00      jeanberube4   \n",
       "\n",
       "                                            TWEET_TEXT  NUMBER_OF_RETWEET  \n",
       "0    @EmmanuelMacron Cessez de mentir. Sortez l’éle...                  0  \n",
       "1    RT @ChamboncelBen: \" C'est inadmissible\"\\n\\nLa...                 25  \n",
       "2    RT @Europhobe: 7 milliards, c'est le prix d'un...                145  \n",
       "3    RT @DigitalGanon: Leurs vieilles ganache de bo...                271  \n",
       "4    RT @Reporterre: Plus de routes ne signifie pas...                 10  \n",
       "..                                                 ...                ...  \n",
       "295  RT @elkevdbrandt: Plus de trains 🚊 , vers et d...                 17  \n",
       "296  RT @f_philippot: Pourquoi dit-on qu’on va manq...               1679  \n",
       "297  RT @JLMTV_INFOS: Bonjour Twitter. \\nHier le mi...                 33  \n",
       "298  RT @f_philippot: La seule raison pour laquelle...                343  \n",
       "299  RT @PatriceRoyTJ: La fonte des glaciers bat de...                  6  \n",
       "\n",
       "[300 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tweets = searchTweetsWordDictionnary(client, word_dictionnary)\n",
    "all_tweet = pd.DataFrame(results)\n",
    "\n",
    "#all_tweet.to_csv('First_Dataset.csv')\n",
    "\n",
    "display(all_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c75fc40",
   "metadata": {},
   "source": [
    "## 2ND DATASET : SEARCH TWEETS FOLLOWING USERS ACTIVITY WITH USER DICTIONNARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036a07bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "def searchTweetsUserAct(username):\n",
    "    \n",
    "    apiAuth = authenticate()\n",
    "    tweets = apiAuth.user_timeline(screen_name = username, include_rts = False, count=10000, tweet_mode=\"extended\" )\n",
    "    \n",
    "    results=[]\n",
    "   \n",
    "     \n",
    "    if not tweets is None and len(tweets) > 0:\n",
    "        for tweetInfo in tweets:\n",
    "            \n",
    "            tweetId = tweetInfo.id\n",
    "            tweetDate = tweetInfo.created_at\n",
    "            tweetCreator = tweetInfo.user.name\n",
    "            tweetText = tweetInfo.full_text\n",
    "            tweetFav = tweetInfo.favorite_count\n",
    "            tweetRet = tweetInfo.retweet_count\n",
    "\n",
    "            line = {'ID': tweetId, 'DATE' : tweetDate, 'CREATOR' : tweetCreator, 'TEXT' : tweetText, 'LIKE' : tweetFav, 'RETWEET' : tweetRet}\n",
    "            results.append(line)\n",
    "            \n",
    "\n",
    "        return results\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "    \n",
    "def search_usersTweets_info():\n",
    "\n",
    "    results_tab= []\n",
    "    \n",
    "    for user in actor_dictionnary:\n",
    "        results_tab.extend(searchTweetsUserAct(user))     \n",
    "        \n",
    "    return results_tab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b39ec84",
   "metadata": {},
   "source": [
    "### PRINT IN A DATAFRAME TO EXPORT 2ND DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290dddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultat2ndDt = search_usersTweets_info()\n",
    "\n",
    "SecData = pd.DataFrame(resultat2ndDt)\n",
    "SecData.to_csv('Second_Dataset.csv',encoding='utf-8-sig')\n",
    "display (SecData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3bcf2c",
   "metadata": {},
   "source": [
    "## 3RD DATASET : SEARCH USERS INFORMATION FROM USER DICTIONNARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "289d3b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchActor(name):\n",
    "    \n",
    "    apiAuth = authenticate()\n",
    "\n",
    "    results=[]\n",
    "    my_followers = []\n",
    "    my_followers_id = []\n",
    "    my_following = []\n",
    "    \n",
    "\n",
    "    for friend in tweepy.Cursor(apiAuth.get_friends, screen_name=name).items():\n",
    "        my_following.append(friend.screen_name)\n",
    "        \n",
    "    for follower in tweepy.Cursor(apiAuth.get_followers, screen_name=name).items():\n",
    "        my_followers.append(follower.screen_name)\n",
    "      \n",
    "    for followerid in tweepy.Cursor(apiAuth.get_follower_ids, screen_name=name).items():\n",
    "        my_followers_id.append(followerid)\n",
    "\n",
    "\n",
    "    user = apiAuth.get_user(screen_name = name)\n",
    "\n",
    "    if not user is None :\n",
    "        userId = user.id\n",
    "        userName = user.screen_name\n",
    "        userFollower = user.followers_count\n",
    "        userFollow = user.friends_count\n",
    "        location = user.location\n",
    "        userVerified = user.verified\n",
    "        userDescription = user.description\n",
    "        userRt_count = user.status.retweet_count\n",
    "        userFav_count = user.status.favorite_count\n",
    "\n",
    "        line = {'ID': userId, 'NAME': userName, 'NB_FOLLOWER': userFollower, 'NB_FOLLOW': userFollow, 'LOCATION': location,\n",
    "        'VERIFIED': userVerified, 'DESCRIPTION': userDescription, 'COUNT_RT': userRt_count, 'COUNT_FAV': userFav_count, \n",
    "        'NAME_FOLLOWERS': my_followers, 'NAME_FOLLOWING': my_following, 'ID_FOLLOWERS': my_followers_id}\n",
    "        results.append(line)\n",
    "\n",
    "    return results\n",
    "   \n",
    "\n",
    "\n",
    "def search_users_info():\n",
    "    listOfActeurs = [\"EmmanuelMacron\"] # \"EmmanuelMacron\" OR \"bcvorwerck\"\n",
    "    resultss = []\n",
    "    \n",
    "    for userName in listOfActeurs:\n",
    "        resultss.extend(searchActor(userName))     \n",
    "        \n",
    "    return resultss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccec980",
   "metadata": {},
   "source": [
    "### PRINT IN A DATAFRAME TO EXPORT 3RD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6878aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 772\n"
     ]
    }
   ],
   "source": [
    "resultat3d = search_users_info()\n",
    "\n",
    "ThrData = pd.DataFrame(resultat3d)\n",
    "ThrData.to_csv('tabUserv.csv',encoding='utf-8-sig')\n",
    "ThrData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b2b6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9206079",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonction qui recherche tous les tweets des acteurs liés à l'écologie\n",
    "\n",
    "resultat_string = []\n",
    "def searchTweetsperActor():    \n",
    "    analyze = []       \n",
    "    \n",
    "    for user in actor_dictionnary:\n",
    "        analyze.extend(searchTweetsUserAct(user))\n",
    "        \n",
    "    return analyze\n",
    "\n",
    "def arrangement_tweet():\n",
    "    \n",
    "    tweet_text_tab = []\n",
    "    corpus = []\n",
    "    ps = PorterStemmer()\n",
    "    stop_words = stopwords.words('french')\n",
    "    \n",
    "    all_tweet_of_ecoactors = searchTweetsperActor()\n",
    "    \n",
    "    ecoactor_tweet_frame = pd.DataFrame(all_tweet_of_ecoactors)\n",
    "    pd.options.display.max_colwidth = 10000000\n",
    "    pd.options.display.max_rows = 1000000000\n",
    "\n",
    "\n",
    "    tweet_text_tab.append(ecoactor_tweet_frame.TEXT)\n",
    "    #print(tweet_text_tab)\n",
    "    \n",
    "#(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|[0-9]\n",
    "    \n",
    "    for i in range (len(tweet_text_tab)):        \n",
    "        arrangedTweet = re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|[0-9]\", ' ', str(tweet_text_tab[i]))       \n",
    "        arrangedTweet = arrangedTweet.lower()        \n",
    "        arrangedTweet = tokenize.sent_tokenize(str(tweet_text_tab[i]))        \n",
    "        #arrangedTweet = arrangedTweet.split()\n",
    "    \n",
    "        #arrangedTweet = [x for x in arrangedTweet if not x in stop_words]\n",
    "        #arrangedTweet = [ps.stem(x) for x in arrangedTweet]\n",
    "        arrangedTweet = \" \".join(arrangedTweet)\n",
    "        \n",
    "        \n",
    "        corpus.append(arrangedTweet)\n",
    "        \n",
    "    sacdemots = []\n",
    "       \n",
    "    for i in range(len(corpus)):\n",
    "        \n",
    "        words = corpus[i].split()\n",
    "        sacdemots.append(words)\n",
    "\n",
    "        all_words = list(itertools.chain(*sacdemots))\n",
    "        resultat_string.append(all_words)\n",
    "        words_freq = collections.Counter(all_words)\n",
    "        createtest = words_freq.most_common(300)\n",
    "    \n",
    "       \n",
    "    return createtest\n",
    "\n",
    "word_list_tweet = arrangement_tweet()\n",
    "\n",
    "\n",
    "frame_list_word_count = pd.DataFrame(word_list_tweet, columns = ['Mots', 'Occurrences'])\n",
    "#frame_list_word_count.to_csv(\"BIGFATAFRAMAA.csv\", encoding=\"utf-8\")\n",
    "#display(frame_list_word_count)\n",
    "#datacss.TEXT = datacss.TEXT.apply(lambda x : x.replace('\\n', '\\\\n'))\n",
    "\n",
    "#datacss.TEXT.to_csv(\"NEWGREATTEST.csv\", encoding=\"utf-8\",  index=False)\n",
    "\n",
    "mask = frame_list_word_count['Mots'].isin(word_dictionnary_wordcloud)\n",
    "\n",
    "#print(mask)\n",
    "\n",
    "\n",
    "\n",
    "#display(frame_list_word_count)\n",
    "#print(str(resultat_string))\n",
    "\n",
    "final_tab = []\n",
    "\n",
    "for each in word_dictionnary:\n",
    "    print(\"DANS LE DICTIONNAIRE : \" + each)\n",
    "    \n",
    "for each in resultat_string:\n",
    "    for eachi in each:\n",
    "        if eachi in word_dictionnary_wordcloud:\n",
    "            print(\"DANS RESULTAT :\", eachi)\n",
    "            final_tab.append(eachi)\n",
    "        \n",
    "text_word = ' '.join(final_tab)    \n",
    "print(\"SARAAAAAAAAAAH\", text_word)\n",
    "   #datacsv = pd.DataFrame(endo)\n",
    "#datacsv.TEXT = datacsv.TEXT.apply(lambda x : x.replace('\\n', '\\\\n'))\n",
    "\n",
    "#testdesarahaenvoyer.to_csv('testaenvoyer2.csv', index=False, encoding='utf-8-sig', escapechar=\"\\n\")\n",
    "\n",
    "#49\n",
    "#96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe457ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "frame_list_word_count[mask].sort_values(by = 'Occurrences').plot.barh(x= 'Mots', y='Occurrences', ax=ax, color =\"green\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d14964e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wordcloud-1.8.1-cp39-cp39-win_amd64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24369c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b86279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchTweetsperActor():    \n",
    "    analyze = []       \n",
    "    \n",
    "    for user in actor_dictionnary:\n",
    "        analyze.extend(searchTweetsUserAct(user))\n",
    "        \n",
    "    return analyze\n",
    "tweet_text_tab = []\n",
    "\n",
    "    \n",
    "all_tweet_of_ecoactors = searchTweetsperActor()\n",
    "    \n",
    "ecoactor_tweet_frame = pd.DataFrame(all_tweet_of_ecoactors)\n",
    "tweet_frame = ecoactor_tweet_frame.TEXT.astype(str)\n",
    "pd.options.display.max_colwidth = 10000000\n",
    "pd.options.display.max_rows = 1000000000\n",
    "\n",
    "tweet_text_tab.append(tweet_frame)\n",
    "    \n",
    "print(tweet_text_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bc70fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wordcloud = WordCloud(background_color = 'white', max_words = 300, collocations=False, width=6000, height=4000).generate(text_word)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f483d9be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a5a056",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
